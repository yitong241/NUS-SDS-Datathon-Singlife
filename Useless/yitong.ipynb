{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pandas \n",
    "#%pip install matplotlib\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17992, 171)\n",
      "(17992, 37256)\n",
      "(17992, 37256)\n",
      "(17992, 50)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "df = pd.read_parquet(filepath)\n",
    "\n",
    "col_to_delete = [\"clntnum\", \"hh_size_est_0\", \"hh_size_est_1\", \"hh_size_est_2\", \"hh_size_est_3\", \"hh_size_est_4\", \"hh_size_est_>4\"]\n",
    "col_to_classify = [\"race_desc\", \"ctrycode_desc\", \"clttype\", \"stat_flag\", \"cltsex_fix\", \"annual_income_est\"]\n",
    "col_to_convert_to_year = [\"min_occ_date\", \"cltdob_fix\"]\n",
    "col_to_target = ['f_purchase_lh']\n",
    "\n",
    "for col in col_to_target:\n",
    "    df[col] = df[col].fillna(0)\n",
    "\n",
    "special_cols = set(col_to_delete + col_to_classify + col_to_convert_to_year + col_to_target)\n",
    "\n",
    "for col in col_to_delete:\n",
    "    if col in df.columns:\n",
    "        df = df.drop(col, axis=1)\n",
    "\n",
    "def convert_dates(df, date_cols):\n",
    "    reference_date = datetime.now()\n",
    "    for col in date_cols:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        df[col] = (reference_date - df[col]).dt.days\n",
    "    return df\n",
    "\n",
    "df = convert_dates(df, col_to_convert_to_year)\n",
    "\n",
    "# change value in hh_size_est to numeric\n",
    "def transform_value(value):\n",
    "    if value == '>4':\n",
    "        return 5\n",
    "    else:\n",
    "        return pd.to_numeric(value, errors='coerce')\n",
    "\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "\n",
    "# for col in col_to_classify:\n",
    "#     df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "nan_threshold = 0.5\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in special_cols:\n",
    "        if df[col].isna().sum() / len(df) > nan_threshold:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "        elif df[col].nunique() == 1:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "data = df \n",
    "data.style\n",
    "print(data.shape)\n",
    "\n",
    "X = data.drop(['f_purchase_lh'], axis=1)\n",
    "y = data['f_purchase_lh']\n",
    "\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "preprocessor = ColumnTransformer( \n",
    "    transformers=[\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    ])\n",
    "\n",
    "X = preprocessor.fit_transform(X)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "X = scaler.fit_transform(X).todense()\n",
    "X = np.asarray(X)\n",
    "print(X.shape)\n",
    "\n",
    "\n",
    "pca = PCA(n_components=50)\n",
    "X = pca.fit_transform(X)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "X_tensor = torch.tensor(X.astype(np.float32))\n",
    "y_tensor = torch.tensor(y.values.astype(np.float32))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14393, 50]) torch.Size([3599, 50]) torch.Size([14393]) torch.Size([3599])\n",
      "tensor([[ -1.7033,   1.0124,   2.3555,  ...,   1.1050,   0.3855,   0.5771],\n",
      "        [ -2.5979,   1.2807,   2.9547,  ...,   0.3847,   0.0950,   0.5005],\n",
      "        [ -1.9386,   0.5229,   1.4258,  ...,   0.1156,   0.3536,  -0.0458],\n",
      "        ...,\n",
      "        [ -2.7158,   1.2625,   2.9351,  ...,   0.5005,   0.0799,   0.0535],\n",
      "        [ 20.3472,  26.0633, -10.2620,  ...,   4.9146,   4.6629,  -4.7992],\n",
      "        [  2.8267,  -1.4507,  -2.2684,  ...,   1.4302,   1.0373,  -0.9946]])\n",
      "tensor([1., 0., 0.,  ..., 0., 1., 0.])\n",
      "tensor([[  5.2431,  -0.7719,  -8.7954,  ..., -19.2601,  -7.3481,   3.1085],\n",
      "        [ -2.2608,  -0.4303,  -1.3652,  ...,  -0.7003,  -0.4196,  -0.3793],\n",
      "        [  1.3416,   0.5008,   1.0997,  ...,  -0.5526,  -0.8995,   2.3992],\n",
      "        ...,\n",
      "        [ -2.2048,   1.1949,   3.0849,  ...,   0.5312,  -0.0786,   0.1920],\n",
      "        [  4.4574,  -1.0873,  -1.5389,  ...,   0.1598,   0.5004,  -0.3446],\n",
      "        [  2.1713,  -1.4430,  -2.4031,  ...,   1.3839,   1.5984,  -0.9546]])\n",
      "tensor([0., 0., 1.,  ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "print(X_test)\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(X_train.shape[1], 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(X_train.shape[1], 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.layer2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.layer3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu1(self.layer1(x)))\n",
    "        x = self.dropout2(self.relu2(self.layer2(x)))\n",
    "        x = self.relu3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0655083656311035\n",
      "Epoch 2, Loss: 1.7683472633361816\n",
      "Epoch 3, Loss: 1.3604263067245483\n",
      "Epoch 4, Loss: 1.58438241481781\n",
      "Epoch 5, Loss: 0.9882317781448364\n",
      "Epoch 6, Loss: 1.0103141069412231\n",
      "Epoch 7, Loss: 1.8322023153305054\n",
      "Epoch 8, Loss: 1.1086745262145996\n",
      "Epoch 9, Loss: 0.6727161407470703\n",
      "Epoch 10, Loss: 0.9953071475028992\n",
      "Epoch 11, Loss: 1.2598339319229126\n",
      "Epoch 12, Loss: 1.4411629438400269\n",
      "Epoch 13, Loss: 1.8651655912399292\n",
      "Epoch 14, Loss: 1.6053541898727417\n",
      "Epoch 15, Loss: 1.9625320434570312\n",
      "Epoch 16, Loss: 0.8541809916496277\n",
      "Epoch 17, Loss: 1.7547931671142578\n",
      "Epoch 18, Loss: 1.0922832489013672\n",
      "Epoch 19, Loss: 1.5468279123306274\n",
      "Epoch 20, Loss: 1.0187641382217407\n",
      "Epoch 21, Loss: 0.9219175577163696\n",
      "Epoch 22, Loss: 1.3447502851486206\n",
      "Epoch 23, Loss: 0.6680570840835571\n",
      "Epoch 24, Loss: 0.8807213306427002\n",
      "Epoch 25, Loss: 0.6638408303260803\n",
      "Epoch 26, Loss: 1.7277990579605103\n",
      "Epoch 27, Loss: 1.0748413801193237\n",
      "Epoch 28, Loss: 1.421732783317566\n",
      "Epoch 29, Loss: 1.1448417901992798\n",
      "Epoch 30, Loss: 0.6470244526863098\n"
     ]
    }
   ],
   "source": [
    "class_counts = data['f_purchase_lh'].value_counts().sort_index().values\n",
    "total_samples = class_counts.sum()\n",
    "weights = total_samples / torch.tensor(class_counts, dtype=torch.float32)\n",
    "\n",
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = weights.to(device)\n",
    "model.to(device)\n",
    "\n",
    "# Modify the BCELoss to handle weights for each batch\n",
    "def weighted_bce_loss(outputs, targets):\n",
    "    loss = nn.BCELoss(reduction='none')(outputs, targets)\n",
    "    weighted_loss = loss * weights[targets.long()]\n",
    "    return weighted_loss.mean()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "for epoch in range(30): \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = weighted_bce_loss(outputs, labels.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.4912475687691%\n"
     ]
    }
   ],
   "source": [
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.squeeze() == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.07372072853425846\n",
      "Recall: 0.5629139072847682\n",
      "F1 Score: 0.1303680981595092\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Function to calculate precision, recall, and F1 score\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Convert test data to DataLoader\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        y_pred.extend(predicted.squeeze().tolist())\n",
    "        y_true.extend(labels.tolist())\n",
    "\n",
    "# Calculate metrics\n",
    "precision, recall, f1 = calculate_metrics(np.array(y_true), np.array(y_pred))\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    result = [] \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
